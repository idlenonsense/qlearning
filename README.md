# Q-Learning: Интерактивная демонстрация для Android

Это Android-приложение представляет собой **интерактивную визуализацию алгоритма Q-обучения** — одного из базовых методов безмодельного обучения с подкреплением (Reinforcement Learning). Проект реализован полностью на **Kotlin** с использованием **Jetpack Compose** и предназначен для наглядного понимания того, как агент обучается оптимальному поведению в дискретной среде **без априорной модели мира**.

## Суть демонстрации

Агент (шахтёр) перемещается по сетке 5×5 с целью достичь клетки с алмазом (награда `+250`), избегая ям (штраф `-100`).  
Изначально агент **ничего не знает** о расположении целей или ловушек. Через итеративное взаимодействие со средой он строит **Q-таблицу** — оценку ценности каждого действия в каждом состоянии — и постепенно формирует оптимальную стратегию.

Процесс обучения следует канонической формуле обновления Q-значений:

\[
Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \Big]
\]

где:
- \( \alpha = 0.98 \) — высокая скорость обучения (быстрая адаптация к новым условиям),
- \( \gamma = 0.6 \) — умеренное дисконтирование будущих наград.

## Возможности приложения

- **Интерактивная сетка**: касанием можно добавлять или удалять ямы (штрафные состояния).
- **Гибкое управление обучением**:
  - **Step** — один шаг с ε = 0.15 (преимущественно эксплуатация).
  - **Episode** — прохождение до конца эпизода (до цели или падения) с ε = 0.5.
  - **Train** — 1000 полных эпизодов обучения (ε = 0.65) для сходимости Q-таблицы.
- **Сброс и рандомизация**: автоматическая расстановка 8 случайных ям.
- **Двухъязычный интерфейс**: переключение между английским и русским языком в реальном времени.

## Техническая реализация

- **Язык**: Kotlin (Android)
- **UI-фреймворк**: Jetpack Compose
- **Архитектура**: 
  - `QLearning` — чистая логика RL-алгоритма (состояние, действия, Q-таблица, политика).
  - `MainActivity` + Composable-функции — декларативный UI, полностью реактивный через `mutableStateOf`.
- **Состояние**: все изменяемые данные (`agentCoordinates`, `penaltyCoordinates`, `info`) обёрнуты в `State` для автоматического перерисовывания UI.
- **Детерминированное окружение**: каждая клетка — уникальное состояние (индекс = `row * 5 + col`).

## Зачем это нужно?

Проект иллюстрирует **фундаментальную идею RL**: обучение через пробу и ошибку, без прямого надзора. В отличие от симуляторов в Jupyter Notebook, здесь вы **физически взаимодействуете с окружением**, меняете его структуру и сразу видите, как это влияет на обучение — что делает абстрактные концепции (`exploration vs exploitation`, `discounted reward`, `policy convergence`) осязаемыми.

## Сборка и запуск

Требования:
- Android Studio (Giraffe или новее)
- Android SDK ≥ 21
- Kotlin 1.9+

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/idlenonsense/qlearning.git
